{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR) in Python using ScikitLearn\n",
    "\n",
    "In this program we predict the salaries based on level position.\n",
    "\n",
    "**Dataset Description**\n",
    "\n",
    "The dataset to this model is composed by three columns and 1o row. We have one feature in the second column, Level Position. Our response is the last column, Salary. Based on the level, we construct a polynomial regression model to predict the salary for a given level position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_csv('Position_Salaries.csv') #Importing the dataset and creating the dataframe\n",
    "X = dataset.iloc[:, 1:-1].values # Defining the independente variable\n",
    "y = dataset.iloc[:, -1].values # Defining the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "In this model we need to apply feature scaling, for one simple reason, the SVR model is not a linear combination between  $X$ and $y$. There is an other trick behind the method. Then we need to have all independent variables in the same scale to make a good prediction. Note that,  in all situation that we have a linear combination between $X$ and $y$ we do not need to apply feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(len(y),1) # To convert this 1D array into 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model - Support Vector Regression (SVR)\n",
    "\n",
    "**Definition from Scikit-Learn web site**\n",
    "\n",
    "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Before proceeding with the feature scaling we must change the array y into a 2D\n",
    "# array. We must do it because the feature scaling object just transforms 2D arrays\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training the SVR model\n",
    "# Due to the correlation between X and y in the dataset, we do not need to split\n",
    "# the dataset into train and test. So here, we are going to train the model on\n",
    "# the whole dataset. We call a new class in scikitlearn libraary, sklearn.svm\n",
    "# and we utilize the object SVR.\n",
    "# In the object SVR we have many arguments, that are model of distributions.\n",
    "# The better distribution model for this method is Hyperbolic Tangent Kernel.\n",
    "# But all dataset is a linear combination and follow a normal distribution.\n",
    "# For this reason, we utilize Gaussian Radial Basis Function (RBF)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='rbf', degree=6)\n",
    "regressor.fit(X,y)\n",
    "\n",
    "# Now we can make some predictions, our model is trained on whole dataset.\n",
    "# Heads-up!!! We can not make a directly prediction we employed a feature scaling.\n",
    "# Before any prediction, we need come back to the original scale.\n",
    "\n",
    "# A single prediction.\n",
    "print(sc_y.inverse_transform(regressor.predict(sc_X.transform([[10]]))))\n",
    "# Like the salary is the dependent variable to be predict, in this single prediction\n",
    "# we used the inverse transformation. But be careful, to visualize the results\n",
    "# you must aplly the the inverse transformation to the all variables.\n",
    "\n",
    "# Visualizing the results\n",
    "X_os = sc_X.inverse_transform(X)\n",
    "y_os = sc_y.inverse_transform(y)\n",
    "\n",
    "plt.title('Truth or Bluff (Support Vector Regression)')\n",
    "plt.scatter(X_os, y_os, color = 'red')\n",
    "# plotting the model with predictions and your transformation\n",
    "plt.plot(X_os, sc_y.inverse_transform(regressor.predict(X)), color = 'blue')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "\n",
    "# Visualising the SVR results (for higher resolution and smoother curve)\n",
    "\n",
    "X_grid = np.arange(min(X_os), max(X_os), 0.001)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "X_grid_os = sc_X.transform(X_grid)\n",
    "\n",
    "plt.scatter(X_os,y_os, color = 'red')\n",
    "plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(X_grid_os)), color = 'blue')\n",
    "plt.title('Truth or Bluff (SVR)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# This program is a simple exemple to demonstre how to use a SVR model.\n",
    "# The model is different to the linear regression and the theory behind the method\n",
    "# is very interesting and aid to understand better. I suggest a quickly read of it.\n",
    "# You most also try to fit a best model using the arguments in the object SVR().\n",
    "# Enjoy machine learn.\n",
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
