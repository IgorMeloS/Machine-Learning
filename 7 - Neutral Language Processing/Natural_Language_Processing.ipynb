{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural_Language_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWWosJhLXFta"
      },
      "source": [
        "## Natural Language Processing in Python \n",
        "This program has as objective to construct a Bag of Words Model a branch of Natural Language Processing.\n",
        "The data-set is composed by many reviews of quality and service of a restaurant. The structure of the data-set is two columns. The first column contains the review and the second the sentiment analysis (0 to negative sentiment and 1 to positive sentiment). We are going to train the model to predict the sentiment of a review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81yAdwgbXWNF"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3kw7L1XXZmT"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvw7DYReX3s5"
      },
      "source": [
        "# Importing Dataset\n",
        "This data-set is composed by two columns and one thousand rows (number of reviews)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE2yT8ieYkx-"
      },
      "source": [
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
        "# Here we employed the delimiter to read a tabular separate and quoting = 3 to suppress all quotes (it can interfere in the model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_XBhYq5aVoy"
      },
      "source": [
        "# Cleaning the Text\n",
        "An important step in Natural Language Processing is to clean your text. It's means that we are going to select the relevant words to take account in our model. For example, words like I, you, Me, this, that, a, an are not relevant to the model. For this reason we utilize Natural Language Toolkit that provides a list with the most non  relevant words and from this list we can eliminate these words from our list of relevant words.\n",
        "An other important process is to stemmer the words, it’s means we just consider the root of the words, for example, loved, lovely will be stemmed to love. One apply it to reduce our sparse matrix. So for the text cleaning we utilize some modulus as NLTK and RE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spOCMbK4a9EF",
        "outputId": "8fc88a1f-8585-49b1-d638-fb355a60b835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re # Regular Expression\n",
        "import nltk # Natural Language Toolkit\n",
        "nltk.download('stopwords') # Stopwords is a list that contains all not relevant words\n",
        "from nltk.corpus import stopwords # Calling Stopwords class to creat the list with non relevant words\n",
        "from nltk.stem.porter import PorterStemmer # The class to stemmer the words\n",
        "corpus = [] # Empty list. We are going to add the cleaned text into this list\n",
        "# This loop for to clean the text row by row\n",
        "for i in range(0, 1000):\n",
        "  # Now we creat a new variable called review, we transform the original text, first we elimate all pontuations and then all capital letters\n",
        "  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # This eliminates all potuation the first [] refers to the column and the second [] to the rows\n",
        "  review = review.lower() # To transform capital letter into lowercase\n",
        "  review = review.split() # We need to split the elements to create a list with many elements, in the past step we had just a list with one single element\n",
        "  ps = PorterStemmer() # The objecto to stemmer the words\n",
        "  all_stopwords = stopwords.words('english') # List that contains all stopwords (non relevant words)\n",
        "  all_stopwords.remove('not') # The word not is inside the stop word, but for this sentiment analysis it is important. We remove from the stopwords list\n",
        "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)] # Here eliminate the stopwords from review and then we transform to the root form\n",
        "  review = ' '.join(review) # We join all the words in each row to respect the original format\n",
        "  corpus.append(review) # Here we create the final list that contians the cleaned text and from this list we are going to construct our model.\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncIta04LFAD9"
      },
      "source": [
        "# Creating the Bag of Words Model\n",
        "\n",
        "This model transforms words into to number and construct a sparse matrix. This matrix contains our independent variables (numbers that represent each word according with you frequency in the text, for example a word that appears two times will have 2 as associate number. Each word will be considered as a column. If a certain word appears in a certain row (text review), in this row we are going to see a number different of zero). Once time that we have the sparse matrix we are able to train our model of sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM8NpUJYIO46"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # This class transform words into a number\n",
        "cv = CountVectorizer(max_features=1500) # This object converts words into number (max_feature is the number of desired word (number of columns of the sparse matrix))\n",
        "X = cv.fit_transform(corpus).toarray() # Here we create our sparse matrix and we transform words into numbers (Tokenization)\n",
        "y = dataset.iloc[:, -1].values # Here we create our dependent variable from the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmcaHLYCLmJ5"
      },
      "source": [
        "# Splitting Dataset into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt3kYCdkLrvP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH6R4-u-LwL2"
      },
      "source": [
        "#  Training all classification models\n",
        "\n",
        "The response to know a new review come from a classification model.\n",
        "Here we have seven different models of classification to train the data-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgiIf7D1L1Pm"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier1 = LogisticRegression(random_state = 0)\n",
        "classifier1.fit(X_train, y_train)\n",
        "\n",
        "# K Nearest Nieghbors\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier2 = KNeighborsClassifier(n_neighbors= 5, metric='minkowski', p = 2)\n",
        "classifier2.fit(X_train, y_train)\n",
        "\n",
        "# Support Vector Machine - Linear Classifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "classifier3 = SVC(kernel= 'linear', random_state=0)\n",
        "classifier3.fit(X_train, y_train)\n",
        "\n",
        "# Kernel Support Vector Machine\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "classifier4 = SVC(kernel='rbf', random_state=0)\n",
        "classifier4.fit(X_train, y_train)\n",
        "\n",
        "# Naïves Bayes Classification\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier5 = GaussianNB()\n",
        "classifier5.fit(X_train, y_train)\n",
        "\n",
        "# Decision Tree Classification\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier6 = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classifier6.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier7 = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state = 0)\n",
        "classifier7.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLLn8ZzMBh0"
      },
      "source": [
        "# Making the Test Results\n",
        "\n",
        "Here we predict the test results for each trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5bj_rtMFz2"
      },
      "source": [
        "y_pred1 = classifier1.predict(X_test)\n",
        "y_pred2 = classifier2.predict(X_test)\n",
        "y_pred3 = classifier3.predict(X_test)\n",
        "y_pred4 = classifier4.predict(X_test)\n",
        "y_pred5 = classifier5.predict(X_test)\n",
        "y_pred6 = classifier5.predict(X_test)\n",
        "y_pred7 = classifier7.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHuIrpomMS4c"
      },
      "source": [
        "# Making Confusion Matrix and Accuracy Score\n",
        "Is important to verify the accuracy score to choose the more apropriate model for this dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ2LSoJNMVS1",
        "outputId": "45319ae7-028a-4751-806f-80d779a36c60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm1 = confusion_matrix(y_test, y_pred1)\n",
        "ac1 = accuracy_score(y_test, y_pred1)\n",
        "cm2 = confusion_matrix(y_test, y_pred2)\n",
        "ac2 = accuracy_score(y_test, y_pred2)\n",
        "cm3 = confusion_matrix(y_test, y_pred3)\n",
        "ac3 = accuracy_score(y_test, y_pred3)\n",
        "cm4 = confusion_matrix(y_test, y_pred4)\n",
        "ac4 = accuracy_score(y_test, y_pred4)\n",
        "cm5 = confusion_matrix(y_test, y_pred5)\n",
        "ac5 = accuracy_score(y_test, y_pred5)\n",
        "cm6 = confusion_matrix(y_test, y_pred6)\n",
        "ac6 = accuracy_score(y_test, y_pred6)\n",
        "cm7 = confusion_matrix(y_test, y_pred7)\n",
        "ac7 = accuracy_score(y_test, y_pred7)\n",
        "print('Checking Confusion Matrix and accuracy score to a single observation')\n",
        "print('Logistic Regression Classification')\n",
        "print(cm1)\n",
        "print(ac1)\n",
        "print('\\n')\n",
        "print('K Nearest Neighbors')\n",
        "print(cm2)\n",
        "print(ac2)\n",
        "print('\\n')\n",
        "print('Support Vector Machine')\n",
        "print(cm3)\n",
        "print(ac3)\n",
        "print('\\n')\n",
        "print('Kernel Support Vector Machine')\n",
        "print(cm4)\n",
        "print(ac4)\n",
        "print('\\n')\n",
        "print('Naive Bayes Classification')\n",
        "print(cm5)\n",
        "print(ac5)\n",
        "print('\\n')\n",
        "print('Decision Tree Classification')\n",
        "print(cm6)\n",
        "print(ac6)\n",
        "print('\\n')\n",
        "print('Random Forest Classification')\n",
        "print(cm7)\n",
        "print(ac7)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking Confusion Matrix and accuracy score to a single observation\n",
            "Logistic Regression Classification\n",
            "[[80 17]\n",
            " [28 75]]\n",
            "0.775\n",
            "\n",
            "\n",
            "K Nearest Neighbors\n",
            "[[74 23]\n",
            " [45 58]]\n",
            "0.66\n",
            "\n",
            "\n",
            "Support Vector Machine\n",
            "[[79 18]\n",
            " [24 79]]\n",
            "0.79\n",
            "\n",
            "\n",
            "Kernel Support Vector Machine\n",
            "[[89  8]\n",
            " [36 67]]\n",
            "0.78\n",
            "\n",
            "\n",
            "Naive Bayes Classification\n",
            "[[55 42]\n",
            " [12 91]]\n",
            "0.73\n",
            "\n",
            "\n",
            "Decision Tree Classification\n",
            "[[55 42]\n",
            " [12 91]]\n",
            "0.73\n",
            "\n",
            "\n",
            "Random Forest Classification\n",
            "[[91  6]\n",
            " [41 62]]\n",
            "0.765\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3fCBx2qMpAj"
      },
      "source": [
        "#  Computing the accuracy with k-Fold Cross Validation\n",
        "K-Fold Cross Validation gives a mean of accuracy for each model, it's one way to better choose the most appropriate model.\n",
        "Here we consider the accuracy mean from 10 repetition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJD6q6YlMqLM",
        "outputId": "1c1eceff-73f4-4c1d-8684-371f90c0ffc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies1 = cross_val_score(estimator = classifier1, X = X_train, y = y_train, cv = 10)\n",
        "accuracies2 = cross_val_score(estimator = classifier2, X = X_train, y = y_train, cv = 10)\n",
        "accuracies3 = cross_val_score(estimator = classifier3, X = X_train, y = y_train, cv = 10)\n",
        "accuracies4 = cross_val_score(estimator = classifier4, X = X_train, y = y_train, cv = 10)\n",
        "accuracies5 = cross_val_score(estimator = classifier5, X = X_train, y = y_train, cv = 10)\n",
        "accuracies6 = cross_val_score(estimator = classifier6, X = X_train, y = y_train, cv = 10)\n",
        "accuracies7 = cross_val_score(estimator = classifier7, X = X_train, y = y_train, cv = 10)\n",
        "\n",
        "print('Checking K-Fold Cross Validation')\n",
        "print('\\n')\n",
        "print('Logistic Regression Classification')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies1.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies1.std()*100))\n",
        "print('\\n')\n",
        "print('K Nearest Neighbors')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies2.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies2.std()*100))\n",
        "print('\\n')\n",
        "print('Support Vector Machine')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies3.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies3.std()*100))\n",
        "print('\\n')\n",
        "print('Kernel Support Vector Machine')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies4.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies4.std()*100))\n",
        "print('\\n')\n",
        "print('Naive Bayes Classification')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies5.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies5.std()*100))\n",
        "print('\\n')\n",
        "print('Decision Tree Classification')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies6.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies6.std()*100))\n",
        "print('\\n')\n",
        "print('Random Forest Classification')\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies7.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies7.std()*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking K-Fold Cross Validation\n",
            "\n",
            "\n",
            "Logistic Regression Classification\n",
            "Accuracy: 80.50 %\n",
            "Standard Deviation: 4.30 %\n",
            "\n",
            "\n",
            "K Nearest Neighbors\n",
            "Accuracy: 70.38 %\n",
            "Standard Deviation: 4.30 %\n",
            "\n",
            "\n",
            "Support Vector Machine\n",
            "Accuracy: 79.62 %\n",
            "Standard Deviation: 3.79 %\n",
            "\n",
            "\n",
            "Kernel Support Vector Machine\n",
            "Accuracy: 79.38 %\n",
            "Standard Deviation: 3.96 %\n",
            "\n",
            "\n",
            "Naive Bayes Classification\n",
            "Accuracy: 67.25 %\n",
            "Standard Deviation: 5.30 %\n",
            "\n",
            "\n",
            "Decision Tree Classification\n",
            "Accuracy: 76.63 %\n",
            "Standard Deviation: 3.45 %\n",
            "\n",
            "\n",
            "Random Forest Classification\n",
            "Accuracy: 78.50 %\n",
            "Standard Deviation: 2.08 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfPCSD7wPafL"
      },
      "source": [
        "# Predict a Single review Positive and Negative\n",
        "\n",
        "Here we make a single prediction to a positive and a negative review.\n",
        "We chose Random Forest Classification because this model presents an accuracy score of 78.50% and a Standard Deviation of 2.08%. We have models with high accuracy in comparison with Random Forest Classification, as Logistic Regression, for example. But the Standard Deviation of Random Forest Classification is more satisfactory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ncxwyIuPgmA",
        "outputId": "9f243dbc-6f6f-4eb5-9d1c-111555edce6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Positive case\n",
        "new_review = 'I love this restaurant so much'\n",
        "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
        "new_review = new_review.lower()\n",
        "new_review = new_review.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
        "new_review = ' '.join(new_review)\n",
        "new_corpus = [new_review]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier7.predict(new_X_test)\n",
        "if new_y_pred == 1:\n",
        "  print('Positive Sentiment')\n",
        "else:\n",
        "  print('Negative Sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ6faKkdPyAg",
        "outputId": "35c14c0e-7297-49b1-9fdc-6ee6fc53a6a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Negative case\n",
        "new_review = 'I hate this restaurant so much'\n",
        "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
        "new_review = new_review.lower()\n",
        "new_review = new_review.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
        "new_review = ' '.join(new_review)\n",
        "new_corpus = [new_review]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier7.predict(new_X_test)\n",
        "if new_y_pred == 1:\n",
        "  print('Positive Sentiment')\n",
        "else:\n",
        "  print('Negative Sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative Sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2uDWsv77_RD"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We worked with a Bag of Words Model and we obtained a satisfactory result.\n",
        "To have a good results we must to make attention in the cleaning text process, because it is one of the most important step in this model. After, we must to choose the better model to train.\n",
        "With these models we were able to create good responses with respect to sentiment analysis (yes or no / 0 or 1)."
      ]
    }
  ]
}