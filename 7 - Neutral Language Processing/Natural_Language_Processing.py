# -*- coding: utf-8 -*-
"""Natural_Language_Processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15RDOwEhBsBXQejRRe-u0UARj5dqTlUIM

**Natural Language Processing in Python using Regular Expression, Natural Language Toolkit and Classification Models**

This program has as objective to construct a Bag of Words Model a branch of Natural Language Processing to make sentiment analysis.

**Problem Description**

The problem consists in take care about a set of reviews, transforming the words into a sparse matrix and than, create a classification model capable to say with a review is a negative or positive sentiment. 

**Dataset Description**

The dataset is composed by many reviews of quality and service of a restaurant. The structure of the dataset is two columns (the review and the sentiment) and 1001 rows, each row is a review from a client. The sentiment analysis is classified as 0 to negative sentiment and 1 to positive sentiment.

# Data preprocessing

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import re # Regular Expression
import nltk # Natural Language Toolkit

"""## Importing Dataset

Here, we consider the delimiter '\t 'to read a tabular separate and quoting = 3 to suppress all quotes (it can interfere in the model).
"""

dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)

"""## Cleaning the Text
An important step in Natural Language Processing is to clean your text. It means, we must to select the relevant words to take account in our model. For example, words like I, you, Me, this, that, a, an and others words are not relevant to the model. For this reason, we utilize Natural Language Toolkit, this modulus provides a list with the most non-relevant words. From the stop-word list, we can eliminate these words and keep just the relevant words.

An other important process is to stemmer the words. We consider the root of the words, for example, words as loved, lovely will be stemmed to love. One apply it to reduce our sparse matrix. So, for the text cleaning we utilize some modulus as NLTK and RE.
"""

nltk.download('stopwords')

from nltk.corpus import stopwords # Calling Stopwords class to creat the list with non relevant words
from nltk.stem.porter import PorterStemmer # The class to stemmer the words

corpus = [] # Empty list. We are going to add the cleaned text into this list

all_stopwords = stopwords.words('english') # List that contains all stopwords (non relevant words)
do_not_stop= ['not', 'didn', 'hasn', 'doesn', 'don', 'hadn', 'haven', 'isn', 'wasn', 'aren', 'couldn', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'weren', 'won',  'wouldn']
# do_not_stop is a list with some words which must be considered due to your negative aspect

# Loop to remove the words inside do_not_stop
for keeps in do_not_stop:
    all_stopwords.remove(keeps)
    

ps = PorterStemmer() # Object ot stemmer the words  
# This loop for to clean the text row by row
for i in range(0, 1000):
  # Now we creat a new variable called review, we transform the original text, first we elimate all pontuations and then all capital letters
    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # This eliminates all potuation the first [] refers to the column and the second [] to the rows
    review = review.lower() # To transform capital letter into lowercase
    review = review.split() # We need to split the elements to create a list with many elements, in the past step we had just a list with one single element
    review = [ps.stem(word) for word in review if not word in set(all_stopwords)] # Here eliminate the stopwords from review and then, we stemmer each word
    review = ' '.join(review) # We join all the words to remake the review
    corpus.append(review) # Here we create the final list that contians the cleaned text and from this list we are going to construct our bag of words model.

"""## Creating the Bag of Words Model

This model assigns to each word one column, these columns are the independent variables to build the model. For example, if a review contains the word love two times, for this review the column for love word will contain this appearance frequency, 2.  Once time that we have the sparse matrix, we are able to train our model of sentiment analysis.
"""

from sklearn.feature_extraction.text import CountVectorizer # This class transform words into columns and takes account the appearence frequency.
cv = CountVectorizer(max_features=1500) # This object converts words into columns (max_feature is the number of desired word (number of columns of the sparse matrix))
X = cv.fit_transform(corpus).toarray() # Here we create our sparse matrix and we transform words into numbers (Tokenization)
y = dataset.iloc[:, -1].values # Here we create our dependent variable from the dataset

"""# Building the classification models

## Splitting Dataset into training and test set
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""##  Training the models

The response to know a new review come from a classification model. Here, we have six different models of classification to train the dataset.
"""

# Logistic Regression
from sklearn.linear_model import LogisticRegression
classifier1 = LogisticRegression(random_state = 0)
classifier1.fit(X_train, y_train)

# K Nearest Nieghbors

from sklearn.neighbors import KNeighborsClassifier
classifier2 = KNeighborsClassifier(n_neighbors= 5, metric='minkowski', p = 2)
classifier2.fit(X_train, y_train)

# Kernel Support Vector Machine

from sklearn.svm import SVC
classifier3 = SVC(kernel='rbf', random_state=0)
classifier3.fit(X_train, y_train)

# Na√Øves Bayes Classification

from sklearn.naive_bayes import GaussianNB
classifier4 = GaussianNB()
classifier4.fit(X_train, y_train)

# Decision Tree Classification

from sklearn.tree import DecisionTreeClassifier
classifier5 = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier5.fit(X_train, y_train)

# Random Forest Classification
from sklearn.ensemble import RandomForestClassifier
classifier6 = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state = 0)
classifier6.fit(X_train, y_train)

"""### Predicting the test result"""

y_pred1 = classifier1.predict(X_test)
y_pred2 = classifier2.predict(X_test)
y_pred3 = classifier3.predict(X_test)
y_pred4 = classifier4.predict(X_test)
y_pred5 = classifier5.predict(X_test)
y_pred6 = classifier6.predict(X_test)

"""## Selection Model

To select the best classification model, we consider some metrics as precision, recall, f1-score and accuracy. To verify with more precision, to each model we employ the cross validation. After the cross validation, we are going to choose the three best models to tune.

### Metrics results without cross validation
"""

from sklearn.metrics import classification_report, confusion_matrix
cm1 = confusion_matrix(y_test, y_pred1)
cm2 = confusion_matrix(y_test, y_pred2)
cm3 = confusion_matrix(y_test, y_pred3)
cm4 = confusion_matrix(y_test, y_pred4)
cm5 = confusion_matrix(y_test, y_pred5)
cm6 = confusion_matrix(y_test, y_pred6)

print('Checking Confusion Matrix to a single observation')
print('Logistic Regression Classification')
print(cm1)
print(classification_report(y_test,y_pred1))
print('\n')
print('K Nearest Neighbors')
print(cm2)
print(classification_report(y_test,y_pred2))
print('\n')
print('Support Vector Machine')
print(cm3)
print(classification_report(y_test,y_pred3))
print('\n')
print('Naive Bayes Classification')
print(cm4)
print(classification_report(y_test,y_pred4))
print('\n')
print('Decision Tree Classification')
print(cm5)
print(classification_report(y_test,y_pred5))
print('\n')
print('Random Forest Classification')
print(cm6)
print(classification_report(y_test,y_pred6))
print('\n')

"""### Metrics results with cross validation"""

scoring = ['accuracy', 'precision', 'recall', 'f1'] # List of scores
from sklearn.model_selection import cross_validate
accuracies1 = cross_validate(estimator = classifier1, X = X_train, y = y_train, cv = 10, scoring = scoring)
accuracies2 = cross_validate(estimator = classifier2, X = X_train, y = y_train, cv = 10, scoring = scoring)
accuracies3 = cross_validate(estimator = classifier3, X = X_train, y = y_train, cv = 10, scoring = scoring)
accuracies4 = cross_validate(estimator = classifier4, X = X_train, y = y_train, cv = 10, scoring = scoring)
accuracies5 = cross_validate(estimator = classifier5, X = X_train, y = y_train, cv = 10, scoring = scoring)
accuracies6 = cross_validate(estimator = classifier6, X = X_train, y = y_train, cv = 10, scoring = scoring)


print('Checking K-Fold Cross Validation')
print('\n')
print('Logistic Regression Classification')
print("Accuracy: {:.2f} %".format(accuracies1['test_accuracy'].mean()*100))
print("Precision: {:.2f} %".format(accuracies1['test_precision'].mean()*100))
print("Recall: {:.2f} %".format(accuracies1['test_recall'].mean()*100))
print("F1-score: {:.2f} %".format(accuracies1['test_f1'].mean()*100))
print('\n')
print('K-Nearest Neighbors')
print("Accuracy: {:.2f} %".format(accuracies2['test_accuracy'].mean()*100))
print("Precision: {:.2f} %".format(accuracies2['test_precision'].mean()*100))
print("Recall: {:.2f} %".format(accuracies2['test_recall'].mean()*100))
print("F1-score: {:.2f} %".format(accuracies2['test_f1'].mean()*100))
print('\n')
print('Support Vector Machine')
print("Accuracy: {:.2f} %".format(accuracies3['test_accuracy'].mean()*100))
print("Precision: {:.2f} %".format(accuracies3['test_precision'].mean()*100))
print("Recall: {:.2f} %".format(accuracies3['test_recall'].mean()*100))
print("F1-score: {:.2f} %".format(accuracies3['test_f1'].mean()*100))
print('\n')
print('Naive Bayes Classification')
print("Accuracy: {:.2f} %".format(accuracies4['test_accuracy'].mean()*100))
print("Precision: {:.2f} %".format(accuracies4['test_precision'].mean()*100))
print("Recall: {:.2f} %".format(accuracies4['test_recall'].mean()*100))
print("F1-score: {:.2f} %".format(accuracies4['test_f1'].mean()*100))
print('\n')
print('Decision Tree Classification')
print("Accuracy: {:.2f} %".format(accuracies5['test_accuracy'].mean()*100))
print("Precision: {:.2f} %".format(accuracies5['test_precision'].mean()*100))
print("Recall: {:.2f} %".format(accuracies5['test_recall'].mean()*100))
print("F1-score: {:.2f} %".format(accuracies5['test_f1'].mean()*100))
print('\n')
print('Random Forest Classification')
print("Accuracy: {:.2f} %".format(accuracies6['test_accuracy'].mean()*100))
print("Precision: {:.2f} %".format(accuracies6['test_precision'].mean()*100))
print("Recall: {:.2f} %".format(accuracies6['test_recall'].mean()*100))
print("F1-score: {:.2f} %".format(accuracies6['test_f1'].mean()*100))
print('\n')

"""## Boosting the model

The final step is to tune the models with GridSearchCV. For this case, we select the three best models. We consider the metrics accuracy and precision to select the best models.

### Setting the hyper-parameters
"""

param_grid1 = {'C' : [0.001, 0.01, 0.1, 1 , 10, 100], 'solver' : ['newton-cg', 'sag', 'saga','lbfgs', 'liblinear' ], 'penalty' : ['l1', 'l2']} # Logistic Regression
param_grid2 = {'C': [0.1, 1, 10, 100], 'gamma': [100, 10, 1, 0.1, 0.01, 0.001], 'degree': [2, 4, 6], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']} # Support Vector Machine
param_grid3 = {'criterion': ['gini', 'entropy'], 'n_estimators': [10, 50, 100, 500, 1000]} # Random Forest

"""### Searching the best parameters and training the models

"""

metrics = {'ACC' : 'accuracy', 'PC' : 'precision', 'REC' : 'recall', 'F1' : 'f1'}

from sklearn.model_selection import GridSearchCV

grid1 = GridSearchCV(classifier1, param_grid1, cv = 10, scoring = metrics, refit = 'ACC', verbose = 4)
grid2 = GridSearchCV(classifier3, param_grid2, cv = 10, scoring = metrics, refit = 'ACC', verbose = 4)
grid3 = GridSearchCV(classifier6, param_grid3, cv = 10, scoring = metrics, refit = 'ACC', verbose = 4)

class_boost_1 = grid1.fit(X_train,y_train)
class_boost_2 = grid2.fit(X_train,y_train)
class_boost_3 = grid3.fit(X_train,y_train)

"""### Predicting with the best parameters"""

boost_predictions1 = class_boost_1.predict(X_test)
boost_predictions2 = class_boost_2.predict(X_test)
boost_predictions3 = class_boost_3.predict(X_test)

"""### Metrics scores and the best parameters

#### Logistic Regression Boosted
"""

print('Logistic Regression Classification - Boosted')
print('\n')
print('Best Parameters')
print(grid1.best_params_)
print('Confusion Matrix')
cmb1 = confusion_matrix(y_test, boost_predictions1)
plt.plot()
sns.heatmap(cmb1, annot=True)
plt.show()
print('Metrics Results')
print("- Accuracy: {:.2f} %".format(grid1.cv_results_['mean_test_ACC'][grid1.best_index_].mean()*100))
print("- Precision: {:.2f} %".format(grid1.cv_results_['mean_test_PC'][grid1.best_index_].mean()*100))
print("- Recall: {:.2f} %".format(grid1.cv_results_['mean_test_REC'][grid1.best_index_].mean()*100))
print("- F1-score: {:.2f} %".format(grid1.cv_results_['mean_test_F1'][grid1.best_index_].mean()*100))
print('\n')

"""#### Support Vector Machine"""

print('Support Vector Machine - Boosted')
print('\n')
print('Best Parameters')
print(grid2.best_params_)
print('Confusion Matrix')
cmb2 = confusion_matrix(y_test, boost_predictions2)
plt.plot()
sns.heatmap(cmb2, annot=True)
plt.show()
print('Metrics Results')
print("- Accuracy: {:.2f} %".format(grid2.cv_results_['mean_test_ACC'][grid2.best_index_].mean()*100))
print("- Precision: {:.2f} %".format(grid2.cv_results_['mean_test_PC'].mean()*100))
print("- Recall: {:.2f} %".format(grid2.cv_results_['mean_test_REC'].mean()*100))
print("- F1-score: {:.2f} %".format(grid2.cv_results_['mean_test_F1'].mean()*100))
print('\n')

"""#### Random Forest Boosted"""

print('Random Forest - Boosted')
print('\n')
print('Best Parameters')
print(grid3.best_params_)
print('Confusion Matrix')
cmb3 = confusion_matrix(y_test, boost_predictions3)
plt.plot()
sns.heatmap(cmb3, annot=True)
plt.show()
print('Metrics Results')
print("- Accuracy: {:.3f} %".format(grid3.cv_results_['mean_test_ACC'][grid3.best_index_].mean()*100))
print("- Precision: {:.3f} %".format(grid3.cv_results_['mean_test_PC'][grid3.best_index_].mean()*100))
print("- Recall: {:.3f} %".format(grid3.cv_results_['mean_test_REC'][grid3.best_index_].mean()*100))
print("- F1-score: {:.3f} %".format(grid3.cv_results_['mean_test_F1'][grid3.best_index_].mean()*100))
print('\n')

"""## Predict a Single review Positive and Negative

Here we make a single prediction to a positive and a negative review.
We chose Logistic Regression Classifier because this model presents an accuracy score of $81.38 \%$ and precision of $83.27 \%$. The Support Vector Machine shows an accuracy big then the Logistic Regression, but the precision score of Support Vector Machine is not satisfactory.

"""

# Positive case
new_review = 'I love this restaurant so much'
new_review = re.sub('[^a-zA-Z]', ' ', new_review)
new_review = new_review.lower()
new_review = new_review.split()
new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]
new_review = ' '.join(new_review)
new_corpus = [new_review]
new_X_test = cv.transform(new_corpus).toarray()
new_y_pred = class_boost_1.predict(new_X_test)
if new_y_pred == 1:
  print('Positive Sentiment')
else:
  print('Negative Sentiment')

# Negative case
new_review = 'I hate this restaurant so much'
new_review = re.sub('[^a-zA-Z]', ' ', new_review)
new_review = new_review.lower()
new_review = new_review.split()
new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]
new_review = ' '.join(new_review)
new_corpus = [new_review]
new_X_test = cv.transform(new_corpus).toarray()
new_y_pred = class_boost_1.predict(new_X_test)
if new_y_pred == 1:
  print('Positive Sentiment')
else:
  print('Negative Sentiment')

"""# Conclusion

In this notebook we have built a Bag of Words Model, with the aid of classification models we are able to make the sentiments analysis. The results are very satisfactory, our best model presents good accuracy and precision, being according with the problem reality.
"""