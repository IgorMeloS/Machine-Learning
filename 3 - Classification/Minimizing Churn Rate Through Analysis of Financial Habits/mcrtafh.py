# -*- coding: utf-8 -*-
"""MCRTAFH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15R5_FkG2WfIJf3zMDfHt6Y8m_x3RryN-

**Minimizing Churn Rate Through Analysis of
Financial Habits**

# Data Preprocessing

## Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Importing the Dataset"""

dataset = pd.read_csv('churn_data.csv')

dataset.shape

dataset.head(10)

dataset.columns

dataset.describe()
# From describe we can see we have missing values

dataset.isna().any() # To verify if there's missing values

dataset.isna().sum() # To count the missing values to each column

# Cleaning the Missing values
dataset = dataset[pd.notnull(dataset['age'])]
dataset = dataset.drop(columns= ['credit_score', 'rewards_earned'])

dataset.shape # we missed two columns, it's not a good signe

"""## Histograms"""

dataset2 = dataset.drop(columns = ['user', 'churn'])
fig = plt.figure(figsize=(15, 12))
plt.suptitle('Histograms of Numerical Columns', fontsize=20)
for i in range(1, dataset2.shape[1] + 1):
    plt.subplot(6, 5, i)
    f = plt.gca()
    f.axes.get_yaxis().set_visible(False)
    f.set_title(dataset2.columns.values[i - 1])

    vals = np.size(dataset2.iloc[:, i - 1].unique())
    
    plt.hist(dataset2.iloc[:, i - 1], bins=vals, color='#3F5D7D')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

"""## Pie Chart
For the biniries values
"""

# Setting biniries valeus into dataset2 
dataset2 = dataset[['housing', 'is_referred', 'app_downloaded',
                    'web_user', 'app_web_user', 'ios_user',
                    'android_user', 'registered_phones', 'payment_type',
                    'waiting_4_loan', 'cancelled_loan',
                    'received_loan', 'rejected_loan', 'zodiac_sign',
                    'left_for_two_month_plus', 'left_for_one_month', 'is_referred']]

# Setting the plot
fig = plt.figure(figsize=(20,10))
plt.suptitle('Pie Chart Distribution', fontsize = 20)
for i in range(1, dataset2.shape[1] + 1):
  plt.subplot(6, 3, i)
  f = plt.gca()
  f.axes.get_yaxis().set_visible(False)
  f.set_title(dataset2.columns.values[i - 1])
  # Setting the biniries values
  values = dataset2.iloc[:, i - 1].value_counts(normalize = True).values # to show the binirie values in parcentage
  index = dataset2.iloc[:, i -1].value_counts(normalize = True).index
  plt.pie(values, labels= index, autopct='%1.1f%%')
  plt.axis('equal')
fig.tight_layout(rect=[0, 0.03, 1, 0.95])

"""## Exploring Uneven Features
As we can see there are some biniries variables that are uneven in your proportions. To avois bias results, we creat a mask to verify the relation of these number with the response variable
"""

dataset[dataset2.waiting_4_loan == 1].churn.value_counts()

dataset[dataset2.rejected_loan == 1].churn.value_counts()

dataset[dataset2.cancelled_loan == 1].churn.value_counts()

dataset[dataset2.received_loan == 1].churn.value_counts()

dataset[dataset2.left_for_one_month == 1].churn.value_counts()

"""## Correlation plot in according with the response variable"""

dataset2.drop(columns = ['housing', 'payment_type',
                         'zodiac_sign']
    ).corrwith(dataset.churn).plot.bar(figsize=(20,10),
              title = 'Correlation with Response variable',
              fontsize = 15, rot = 45,
              grid = True)

"""## Correlation Matrix between all numerics variables"""

sns.set(style="white")

# Compute the correlation matrix
corr = dataset.drop(columns = ['user', 'churn']).corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(30, 20))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot = True)

# Removing Correleted field
dataset = dataset.drop(columns = ['app_web_user'])

# Savind the new dataset
dataset.to_csv('new_churn_data.csv', index_label=False)

"""# Model Building

## Importing the new dataset
"""

dataset_model = pd.read_csv('new_churn_data.csv')

dataset_model.shape

"""## Separating user column from the dataset"""

user_identifier = dataset_model['user']
dataset_model = dataset_model.drop(columns=['user'])

"""## One-Hot-Encoding"""

dataset_model.housing.value_counts()

dataset_model.groupby('housing')['churn'].nunique().reset_index()

dataset_model = pd.get_dummies(dataset_model)

dataset_model.columns

dataset_model = dataset_model.drop(columns=['zodiac_sign_na', 'housing_na', 'payment_type_na'])

dataset_model.shape

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(dataset_model.drop(columns=['churn']), dataset_model['churn'], test_size = 0.2, random_state = 0)

X_train.shape

17880 + 5400

X_test.shape

y_train.value_counts()

"""## Balancing the training set (This can change the reality of the model -GRANDE MERDA PODE SER FEITA NESSA PORRA)"""

pos_index = y_train[y_train.values == 1].index
print(pos_index)

neg_index = y_train[y_train.values == 0].index
print(neg_index)

if len(pos_index) > len(neg_index):
    higher = pos_index
    lower = neg_index
else:
    higher = neg_index
    lower = pos_index

higher.shape

lower.shape

import random
random.seed(0)
higher = np.random.choice(higher, size=len(lower))
lower = np.asarray(lower)
new_indexes = np.concatenate((lower, higher))

X_train = X_train.loc[new_indexes,]
y_train = y_train[new_indexes]

y_train.shape

y_train.value_counts()

"""## Feature scaling"""

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))
X_test2 = pd.DataFrame(sc_X.transform(X_test))
X_train2.columns = X_train.columns.values
X_test2.columns = X_test.columns.values
X_train2.index = X_train.index.values
X_test2.index = X_test.index.values
X_train = X_train2
X_test = X_test2

"""## Constructing the models"""

# Logistic Regression
from sklearn.linear_model import LogisticRegression
classifier1 = LogisticRegression(random_state = 0)
classifier1.fit(X_train, y_train)

# K Nearest Nieghbors

from sklearn.neighbors import KNeighborsClassifier
classifier2 = KNeighborsClassifier(n_neighbors= 5, metric='minkowski', p = 2)
classifier2.fit(X_train, y_train)

# Support Vector Machine - Linear Classifier

from sklearn.svm import SVC
classifier3 = SVC(kernel= 'linear', random_state=0)
classifier3.fit(X_train, y_train)

# Kernel Support Vector Machine

from sklearn.svm import SVC
classifier4 = SVC(kernel='rbf', random_state=0)
classifier4.fit(X_train, y_train)

# Na√Øves Bayes Classification

from sklearn.naive_bayes import GaussianNB
classifier5 = GaussianNB()
classifier5.fit(X_train, y_train)

# Decision Tree Classification

from sklearn.tree import DecisionTreeClassifier
classifier6 = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier6.fit(X_train, y_train)

# Random Forest Classification
from sklearn.ensemble import RandomForestClassifier
classifier7 = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state = 0)
classifier7.fit(X_train, y_train)

"""## Predicting new results"""

y_pred1 = classifier1.predict(X_test)
y_pred2 = classifier2.predict(X_test)
y_pred3 = classifier3.predict(X_test)
y_pred4 = classifier4.predict(X_test)
y_pred5 = classifier5.predict(X_test)
y_pred6 = classifier5.predict(X_test)
y_pred7 = classifier7.predict(X_test)

"""## Selection Model

### Confusion Matrix and accuracies response
"""

from sklearn.metrics import classification_report, confusion_matrix
cm1 = confusion_matrix(y_test, y_pred1)
cm2 = confusion_matrix(y_test, y_pred2)
cm3 = confusion_matrix(y_test, y_pred3)
cm4 = confusion_matrix(y_test, y_pred4)
cm5 = confusion_matrix(y_test, y_pred5)
cm6 = confusion_matrix(y_test, y_pred6)
cm7 = confusion_matrix(y_test, y_pred7)
print('Checking Confusion Matrix to a single observation')
print('Logistic Regression Classification')
print(cm1)
print(classification_report(y_test,y_pred1))
print('\n')
print('K Nearest Neighbors')
print(cm2)
print(classification_report(y_test,y_pred2))
print('\n')
print('Support Vector Machine')
print(cm3)
print(classification_report(y_test,y_pred3))
print('\n')
print('Kernel Support Vector Machine')
print(cm4)
print(classification_report(y_test,y_pred4))
print('\n')
print('Naive Bayes Classification')
print(cm5)
print(classification_report(y_test,y_pred5))
print('\n')
print('Decision Tree Classification')
print(cm6)
print(classification_report(y_test,y_pred6))
print('\n')
print('Random Forest Classification')
print(cm7)
print(classification_report(y_test,y_pred7))
print('\n')

"""### K-Fold cross validation"""

from sklearn.model_selection import cross_val_score
accuracies1 = cross_val_score(estimator = classifier1, X = X_train, y = y_train, cv = 10)
accuracies2 = cross_val_score(estimator = classifier2, X = X_train, y = y_train, cv = 10)
accuracies3 = cross_val_score(estimator = classifier3, X = X_train, y = y_train, cv = 10)
accuracies4 = cross_val_score(estimator = classifier4, X = X_train, y = y_train, cv = 10)
accuracies5 = cross_val_score(estimator = classifier5, X = X_train, y = y_train, cv = 10)
accuracies6 = cross_val_score(estimator = classifier6, X = X_train, y = y_train, cv = 10)
accuracies7 = cross_val_score(estimator = classifier7, X = X_train, y = y_train, cv = 10)

print('Checking K-Fold Cross Validation')
print('\n')
print('Logistic Regression Classification')
print("Accuracy: {:.2f} %".format(accuracies1.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies1.std()*100))
print('\n')
print('K Nearest Neighbors')
print("Accuracy: {:.2f} %".format(accuracies2.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies2.std()*100))
print('\n')
print('Support Vector Machine')
print("Accuracy: {:.2f} %".format(accuracies3.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies3.std()*100))
print('\n')
print('Kernel Support Vector Machine')
print("Accuracy: {:.2f} %".format(accuracies4.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies4.std()*100))
print('\n')
print('Naive Bayes Classification')
print("Accuracy: {:.2f} %".format(accuracies5.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies5.std()*100))
print('\n')
print('Decision Tree Classification')
print("Accuracy: {:.2f} %".format(accuracies6.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies6.std()*100))
print('\n')
print('Random Forest Classification')
print("Accuracy1: {:.2f} %".format(accuracies7.mean()*100))
print("Standard1 Deviation: {:.2f} %".format(accuracies7.std()*100))

"""## Boosting the model"""

param_grid1 = {'criterion': ['gini', 'entropy'], 'splitter' : ['best', 'random']}
param_grid2 = {'criterion': ['gini', 'entropy'], 'n_estimators': [10, 50, 100, 500, 1000]}

from sklearn.model_selection import GridSearchCV

grid1 = GridSearchCV(classifier6, param_grid1, cv = 10, verbose = 4)
grid2 = GridSearchCV(classifier7, param_grid2, cv = 10, verbose = 4)

class_boost_1 = grid1.fit(X_train,y_train)
class_boost_2 = grid2.fit(X_train,y_train)